{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJFp0Uk+u5zK12uHA4XEzs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rishu5kumar/Word_Embeddings_using_Keras_Embedding_Layer/blob/main/Training_Word_Embeddings_Using_Embedding_Layer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Embedding Techniques using Embedding Layer in Keras"
      ],
      "metadata": {
        "id": "GjYtKDiJOuZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding layer creates a feature representation of any specific word."
      ],
      "metadata": {
        "id": "Z80bBlGjNi7i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FicyOmjkMUJd"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### sentences\n",
        "sent=['the glass of milk',\n",
        "     'the glass of juice',\n",
        "     'the cup of tea',\n",
        "    'I am a good boy',\n",
        "     'I am a good developer',\n",
        "     'understand the meaning of words',\n",
        "     'your videos are good',]"
      ],
      "metadata": {
        "id": "EbRdzTZHO4rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Vocabulary size, making vector of 10000 vocab for each word\n",
        "voc_size=10000\n",
        "\n",
        "onehot_repr=[tf.keras.preprocessing.text.one_hot(words,voc_size)for words in sent]\n",
        "print(onehot_repr)\n",
        "\n",
        "\n",
        "'''\n",
        "One-Hot Encoding Process\n",
        "Vocabulary Size: You’ve defined a variable voc_size which indicates the total number of unique words you want to encode. For example, let’s assume voc_size is set to 10,000.\n",
        "\n",
        "One-Hot Encoding:\n",
        "The one_hot function from tf.keras.preprocessing.text takes each sentence from the sent list and converts it into a list of integers where each integer corresponds to a unique word in the vocabulary.\n",
        "Each word in a sentence is assigned a unique integer based on its index in the vocabulary.\n",
        "\n",
        "List Comprehension:\n",
        "The line of code is a list comprehension that applies the one_hot function to each sentence in the sent list.\n",
        "It generates a list of one-hot encoded vectors for each sentence.\n",
        "\n",
        "Here’s how the processing works step-by-step:\n",
        "\n",
        "Step 1: Tokenization\n",
        "Each sentence is split into words.\n",
        "Let's assume the following unique words are identified from the sentences:\n",
        "\"the\", \"glass\", \"of\", \"milk\", \"juice\", \"cup\", \"tea\", \"I\", \"am\", \"a\", \"good\", \"boy\", \"developer\", \"understand\", \"meaning\", \"words\", \"your\", \"videos\", \"are\"\n",
        "\n",
        "Step 2: Unique Word Mapping\n",
        "A unique integer is assigned to each word.\n",
        "\n",
        "Step 3: One-Hot Encoding\n",
        "Each sentence is converted to a one-hot encoded representation based on the mapping.\n",
        "For example:\n",
        "\"the glass of milk\" might be converted to [1, 2, 3, 4]\n",
        "\"the glass of juice\" might be converted to [1, 2, 3, 5]\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvU9sWrwO7rH",
        "outputId": "e7225057-0706-4247-9a4e-d200653bffde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[8341, 9030, 6657, 7169], [8341, 9030, 6657, 4069], [8341, 7816, 6657, 2073], [6209, 5317, 6458, 4081, 6168], [6209, 5317, 6458, 4081, 6556], [6742, 8341, 4803, 6657, 8730], [2236, 9678, 8151, 4081]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output [[6654, 998, 8966, 1609]] represents the one-hot encoded integer representation of the words in the sentence sent. Each number is an index (within a vocabulary of size 10,000) corresponding to a word in the sentence. This encoding is based on a hashing mechanism that converts words to unique integers but does not represent actual one-hot vectors.\n",
        "\n",
        "Here we are getting indexes from the dictionary."
      ],
      "metadata": {
        "id": "m0GQ1ODbQA6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "qA1wTJm3PFSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_length=8\n",
        "embedded_docs=tf.keras.preprocessing.sequence.pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\n",
        "print(embedded_docs)\n",
        "\n",
        "'''\n",
        "This line pads the sequences (onehot_repr) to ensure they all have the same length (sent_length):\n",
        "\n",
        "onehot_repr is the list of one-hot encoded sequences.\n",
        "padding='pre' means the padding will be added to the beginning of sequences if they're shorter than sent_length.\n",
        "maxlen=sent_length ensures all sequences are exactly sent_length long.\n",
        "It standardizes the input length for the model.\n",
        "\n",
        "The sequences are padded with zeros at the beginning (because padding='pre') to ensure each one is exactly 5 elements long.\n",
        "\n",
        "This prepares the data for input into a neural network, which typically requires fixed-length input.\n",
        "EX:- onehot_repr = [\n",
        "    [3, 1, 6, 2],\n",
        "    [6, 5, 7],\n",
        "    [8, 4, 5, 7, 2]\n",
        "]\n",
        "then, embedded_docs = [\n",
        "    [0, 0, 3, 1, 6],\n",
        "    [0, 0, 0, 6, 5],\n",
        "    [8, 4, 5, 7, 2]\n",
        "], if sent_length = 5\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "LkWYmePHPJI4",
        "outputId": "a33eece0-f0b1-46ce-bcda-8864805ee6d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   0    0    0    0 8341 9030 6657 7169]\n",
            " [   0    0    0    0 8341 9030 6657 4069]\n",
            " [   0    0    0    0 8341 7816 6657 2073]\n",
            " [   0    0    0 6209 5317 6458 4081 6168]\n",
            " [   0    0    0 6209 5317 6458 4081 6556]\n",
            " [   0    0    0 6742 8341 4803 6657 8730]\n",
            " [   0    0    0    0 2236 9678 8151 4081]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nThis line pads the sequences (onehot_repr) to ensure they all have the same length (sent_length):\\n\\nonehot_repr is the list of one-hot encoded sequences.\\npadding='pre' means the padding will be added to the beginning of sequences if they're shorter than sent_length.\\nmaxlen=sent_length ensures all sequences are exactly sent_length long.\\nIt standardizes the input length for the model.\\n\\nThe sequences are padded with zeros at the beginning (because padding='pre') to ensure each one is exactly 5 elements long.\\n\\nThis prepares the data for input into a neural network, which typically requires fixed-length input.\\nEX:- onehot_repr = [\\n    [3, 1, 6, 2],\\n    [6, 5, 7],\\n    [8, 4, 5, 7, 2]\\n]\\nthen, embedded_docs = [\\n    [0, 0, 3, 1, 6],\\n    [0, 0, 0, 6, 5],\\n    [8, 4, 5, 7, 2]\\n], if sent_length = 5\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whenever we pass anything to the embedding layer, all the sentences should have same number of words as it helps us to create a very good embedding matrix, hence we are using pad_sequences."
      ],
      "metadata": {
        "id": "UMDO4IAkUDpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(voc_size,10,input_length=sent_length))\n",
        "\n",
        "'''\n",
        "This line adds an embedding layer to the model, where:\n",
        "\n",
        "voc_size is the size of the vocabulary (e.g., 10,000).\n",
        "10 is the dimension of the dense embedding vectors (each word is represented as a 10-dimensional vector).\n",
        "input_length=sent_length specifies the length of the input sequences (i.e., how many words per input sentence).\n",
        "\n",
        "This layer converts word indices (from one-hot encoding) into dense vectors of fixed size (10 in this case).\n",
        "'''\n",
        "\n",
        "model.compile('adam','mse')\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "tCtfy0U9PNLh",
        "outputId": "52cbc4ec-95ce-4f89-cc16-fad35d4081c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.predict(embedded_docs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0MRnHq2PP3G",
        "outputId": "f8bcca49-269c-4d23-a407-40a4ce3ea836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
            "[[[-2.2126103e-02  4.5900952e-02  1.1782099e-02  1.2308203e-02\n",
            "   -6.6900142e-03  3.2391716e-02 -3.2273054e-02 -1.6047001e-02\n",
            "   -5.6654923e-03  2.1574821e-02]\n",
            "  [-2.2126103e-02  4.5900952e-02  1.1782099e-02  1.2308203e-02\n",
            "   -6.6900142e-03  3.2391716e-02 -3.2273054e-02 -1.6047001e-02\n",
            "   -5.6654923e-03  2.1574821e-02]\n",
            "  [-2.2126103e-02  4.5900952e-02  1.1782099e-02  1.2308203e-02\n",
            "   -6.6900142e-03  3.2391716e-02 -3.2273054e-02 -1.6047001e-02\n",
            "   -5.6654923e-03  2.1574821e-02]\n",
            "  [-2.2126103e-02  4.5900952e-02  1.1782099e-02  1.2308203e-02\n",
            "   -6.6900142e-03  3.2391716e-02 -3.2273054e-02 -1.6047001e-02\n",
            "   -5.6654923e-03  2.1574821e-02]\n",
            "  [ 1.9436333e-02  4.3055002e-02  3.7182570e-03 -1.4660060e-02\n",
            "    4.1585598e-02  2.2574726e-02  4.4214595e-02  3.7256096e-02\n",
            "   -4.1778814e-02  3.2539282e-02]\n",
            "  [ 4.7121000e-02 -3.6291622e-02  6.0387477e-03  3.3755932e-02\n",
            "   -3.1792477e-02  4.0713739e-02  4.4197328e-03 -1.8629491e-02\n",
            "    3.2509077e-02  1.8076189e-03]\n",
            "  [ 1.0624625e-02 -4.4932079e-02 -2.3287093e-02 -3.7299931e-02\n",
            "    2.0374570e-02  2.2841636e-02  4.2220663e-02  3.5888445e-02\n",
            "    4.0603112e-02 -2.5415480e-02]\n",
            "  [ 4.5282152e-02 -1.9769227e-02 -1.6499616e-02  3.9623346e-02\n",
            "   -1.3142109e-02 -1.2476038e-02 -1.8110491e-02 -4.2774428e-02\n",
            "   -3.7265934e-02 -3.0412281e-02]]\n",
            "\n",
            " [[-2.2126103e-02  4.5900952e-02  1.1782099e-02  1.2308203e-02\n",
            "   -6.6900142e-03  3.2391716e-02 -3.2273054e-02 -1.6047001e-02\n",
            "   -5.6654923e-03  2.1574821e-02]\n",
            "  [-2.2126103e-02  4.5900952e-02  1.1782099e-02  1.2308203e-02\n",
            "   -6.6900142e-03  3.2391716e-02 -3.2273054e-02 -1.6047001e-02\n",
            "   -5.6654923e-03  2.1574821e-02]\n",
            "  [-2.2126103e-02  4.5900952e-02  1.1782099e-02  1.2308203e-02\n",
            "   -6.6900142e-03  3.2391716e-02 -3.2273054e-02 -1.6047001e-02\n",
            "   -5.6654923e-03  2.1574821e-02]\n",
            "  [-2.2126103e-02  4.5900952e-02  1.1782099e-02  1.2308203e-02\n",
            "   -6.6900142e-03  3.2391716e-02 -3.2273054e-02 -1.6047001e-02\n",
            "   -5.6654923e-03  2.1574821e-02]\n",
            "  [ 1.9436333e-02  4.3055002e-02  3.7182570e-03 -1.4660060e-02\n",
            "    4.1585598e-02  2.2574726e-02  4.4214595e-02  3.7256096e-02\n",
            "   -4.1778814e-02  3.2539282e-02]\n",
            "  [ 4.7121000e-02 -3.6291622e-02  6.0387477e-03  3.3755932e-02\n",
            "   -3.1792477e-02  4.0713739e-02  4.4197328e-03 -1.8629491e-02\n",
            "    3.2509077e-02  1.8076189e-03]\n",
            "  [ 1.0624625e-02 -4.4932079e-02 -2.3287093e-02 -3.7299931e-02\n",
            "    2.0374570e-02  2.2841636e-02  4.2220663e-02  3.5888445e-02\n",
            "    4.0603112e-02 -2.5415480e-02]\n",
            "  [-4.8928548e-02  1.1673592e-02  1.0645904e-02  5.1209219e-03\n",
            "   -4.0707421e-02  3.0565884e-02  4.3989900e-02 -6.6586733e-03\n",
            "   -4.9446154e-02 -2.1721173e-02]]\n",
            "\n",
            " [[-2.2126103e-02  4.5900952e-02  1.1782099e-02  1.2308203e-02\n",
            "   -6.6900142e-03  3.2391716e-02 -3.2273054e-02 -1.6047001e-02\n",
            "   -5.6654923e-03  2.1574821e-02]\n",
            "  [-2.2126103e-02  4.5900952e-02  1.1782099e-02  1.2308203e-02\n",
            "   -6.6900142e-03  3.2391716e-02 -3.2273054e-02 -1.6047001e-02\n",
            "   -5.6654923e-03  2.1574821e-02]\n",
            "  [-2.2126103e-02  4.5900952e-02  1.1782099e-02  1.2308203e-02\n",
            "   -6.6900142e-03  3.2391716e-02 -3.2273054e-02 -1.6047001e-02\n",
            "   -5.6654923e-03  2.1574821e-02]\n",
            "  [-2.2126103e-02  4.5900952e-02  1.1782099e-02  1.2308203e-02\n",
            "   -6.6900142e-03  3.2391716e-02 -3.2273054e-02 -1.6047001e-02\n",
            "   -5.6654923e-03  2.1574821e-02]\n",
            "  [ 1.9436333e-02  4.3055002e-02  3.7182570e-03 -1.4660060e-02\n",
            "    4.1585598e-02  2.2574726e-02  4.4214595e-02  3.7256096e-02\n",
            "   -4.1778814e-02  3.2539282e-02]\n",
            "  [ 4.0973831e-02 -8.7463483e-03 -2.0039666e-02  4.1676808e-02\n",
            "   -2.1993741e-03  1.4420040e-03  2.0281207e-02 -4.3773651e-02\n",
            "    4.2756248e-02 -5.9354194e-03]\n",
            "  [ 1.0624625e-02 -4.4932079e-02 -2.3287093e-02 -3.7299931e-02\n",
            "    2.0374570e-02  2.2841636e-02  4.2220663e-02  3.5888445e-02\n",
            "    4.0603112e-02 -2.5415480e-02]\n",
            "  [-1.6524971e-02 -3.4232069e-02 -5.6182034e-03  2.5118198e-02\n",
            "   -2.2754861e-02 -9.3219057e-03  4.1079689e-02  2.1488454e-02\n",
            "    2.6785400e-02  3.3295166e-02]]\n",
            "\n",
            " [[-2.2126103e-02  4.5900952e-02  1.1782099e-02  1.2308203e-02\n",
            "   -6.6900142e-03  3.2391716e-02 -3.2273054e-02 -1.6047001e-02\n",
            "   -5.6654923e-03  2.1574821e-02]\n",
            "  [-2.2126103e-02  4.5900952e-02  1.1782099e-02  1.2308203e-02\n",
            "   -6.6900142e-03  3.2391716e-02 -3.2273054e-02 -1.6047001e-02\n",
            "   -5.6654923e-03  2.1574821e-02]\n",
            "  [-2.2126103e-02  4.5900952e-02  1.1782099e-02  1.2308203e-02\n",
            "   -6.6900142e-03  3.2391716e-02 -3.2273054e-02 -1.6047001e-02\n",
            "   -5.6654923e-03  2.1574821e-02]\n",
            "  [ 2.4197888e-02 -2.4911677e-02  1.8665802e-02 -2.6450753e-02\n",
            "    4.7860552e-02 -4.5422792e-02  6.4463541e-04 -4.6501875e-02\n",
            "   -2.3826122e-02  4.1860748e-02]\n",
            "  [-1.7899051e-03 -4.1601468e-02  2.3217294e-02 -3.9480232e-02\n",
            "   -2.5450587e-03 -3.5144687e-03 -4.4767510e-02  4.9828555e-02\n",
            "    3.9963674e-02 -7.8182109e-03]\n",
            "  [ 9.9274889e-03 -2.9667532e-02 -7.1420297e-03 -4.2637695e-02\n",
            "   -4.2047918e-02 -2.1451473e-02 -9.7292177e-03  7.4410439e-03\n",
            "    2.1505211e-02  3.5159696e-02]\n",
            "  [ 2.5185123e-03  3.2774281e-02  4.9799684e-02  4.9093854e-02\n",
            "   -3.5421669e-02  1.2114607e-02 -1.3848700e-02 -1.9787991e-02\n",
            "   -5.8520436e-03  3.2433782e-02]\n",
            "  [-2.1126999e-02  4.5839064e-03 -4.3659210e-03  5.3205341e-04\n",
            "   -2.4305154e-02  5.6794882e-03  1.1258710e-02 -1.4611471e-02\n",
            "   -4.8128117e-02  4.5580629e-02]]\n",
            "\n",
            " [[-2.2126103e-02  4.5900952e-02  1.1782099e-02  1.2308203e-02\n",
            "   -6.6900142e-03  3.2391716e-02 -3.2273054e-02 -1.6047001e-02\n",
            "   -5.6654923e-03  2.1574821e-02]\n",
            "  [-2.2126103e-02  4.5900952e-02  1.1782099e-02  1.2308203e-02\n",
            "   -6.6900142e-03  3.2391716e-02 -3.2273054e-02 -1.6047001e-02\n",
            "   -5.6654923e-03  2.1574821e-02]\n",
            "  [-2.2126103e-02  4.5900952e-02  1.1782099e-02  1.2308203e-02\n",
            "   -6.6900142e-03  3.2391716e-02 -3.2273054e-02 -1.6047001e-02\n",
            "   -5.6654923e-03  2.1574821e-02]\n",
            "  [ 2.4197888e-02 -2.4911677e-02  1.8665802e-02 -2.6450753e-02\n",
            "    4.7860552e-02 -4.5422792e-02  6.4463541e-04 -4.6501875e-02\n",
            "   -2.3826122e-02  4.1860748e-02]\n",
            "  [-1.7899051e-03 -4.1601468e-02  2.3217294e-02 -3.9480232e-02\n",
            "   -2.5450587e-03 -3.5144687e-03 -4.4767510e-02  4.9828555e-02\n",
            "    3.9963674e-02 -7.8182109e-03]\n",
            "  [ 9.9274889e-03 -2.9667532e-02 -7.1420297e-03 -4.2637695e-02\n",
            "   -4.2047918e-02 -2.1451473e-02 -9.7292177e-03  7.4410439e-03\n",
            "    2.1505211e-02  3.5159696e-02]\n",
            "  [ 2.5185123e-03  3.2774281e-02  4.9799684e-02  4.9093854e-02\n",
            "   -3.5421669e-02  1.2114607e-02 -1.3848700e-02 -1.9787991e-02\n",
            "   -5.8520436e-03  3.2433782e-02]\n",
            "  [-1.8280745e-04 -1.9532299e-02 -2.1565890e-02 -2.6865100e-02\n",
            "    2.9518548e-02  4.6265721e-03  4.4768121e-02 -4.1266531e-04\n",
            "   -4.7451403e-02 -3.7439764e-02]]\n",
            "\n",
            " [[-2.2126103e-02  4.5900952e-02  1.1782099e-02  1.2308203e-02\n",
            "   -6.6900142e-03  3.2391716e-02 -3.2273054e-02 -1.6047001e-02\n",
            "   -5.6654923e-03  2.1574821e-02]\n",
            "  [-2.2126103e-02  4.5900952e-02  1.1782099e-02  1.2308203e-02\n",
            "   -6.6900142e-03  3.2391716e-02 -3.2273054e-02 -1.6047001e-02\n",
            "   -5.6654923e-03  2.1574821e-02]\n",
            "  [-2.2126103e-02  4.5900952e-02  1.1782099e-02  1.2308203e-02\n",
            "   -6.6900142e-03  3.2391716e-02 -3.2273054e-02 -1.6047001e-02\n",
            "   -5.6654923e-03  2.1574821e-02]\n",
            "  [-2.8292144e-02 -4.4901170e-02 -4.2924702e-02  1.1861324e-05\n",
            "    4.5248378e-02 -3.0424261e-02 -3.3395782e-02 -4.9700081e-02\n",
            "    3.1427335e-02 -6.2384978e-03]\n",
            "  [ 1.9436333e-02  4.3055002e-02  3.7182570e-03 -1.4660060e-02\n",
            "    4.1585598e-02  2.2574726e-02  4.4214595e-02  3.7256096e-02\n",
            "   -4.1778814e-02  3.2539282e-02]\n",
            "  [ 1.2439143e-02  3.9591361e-02 -1.5449453e-02  2.8434958e-02\n",
            "    4.2370558e-03  4.1070167e-02 -4.6264734e-02  1.6787793e-02\n",
            "   -1.1792719e-02 -1.1468150e-02]\n",
            "  [ 1.0624625e-02 -4.4932079e-02 -2.3287093e-02 -3.7299931e-02\n",
            "    2.0374570e-02  2.2841636e-02  4.2220663e-02  3.5888445e-02\n",
            "    4.0603112e-02 -2.5415480e-02]\n",
            "  [ 3.7090156e-02  3.4403052e-02  2.1703389e-02  2.9970054e-02\n",
            "   -9.6349940e-03 -4.1230369e-02  4.4413958e-02 -1.1553802e-02\n",
            "    2.5281537e-02  4.4108961e-02]]\n",
            "\n",
            " [[-2.2126103e-02  4.5900952e-02  1.1782099e-02  1.2308203e-02\n",
            "   -6.6900142e-03  3.2391716e-02 -3.2273054e-02 -1.6047001e-02\n",
            "   -5.6654923e-03  2.1574821e-02]\n",
            "  [-2.2126103e-02  4.5900952e-02  1.1782099e-02  1.2308203e-02\n",
            "   -6.6900142e-03  3.2391716e-02 -3.2273054e-02 -1.6047001e-02\n",
            "   -5.6654923e-03  2.1574821e-02]\n",
            "  [-2.2126103e-02  4.5900952e-02  1.1782099e-02  1.2308203e-02\n",
            "   -6.6900142e-03  3.2391716e-02 -3.2273054e-02 -1.6047001e-02\n",
            "   -5.6654923e-03  2.1574821e-02]\n",
            "  [-2.2126103e-02  4.5900952e-02  1.1782099e-02  1.2308203e-02\n",
            "   -6.6900142e-03  3.2391716e-02 -3.2273054e-02 -1.6047001e-02\n",
            "   -5.6654923e-03  2.1574821e-02]\n",
            "  [-1.6834177e-02 -4.7291409e-02 -1.1876561e-02  3.9283264e-02\n",
            "    3.2457482e-02 -3.3802666e-02 -1.3608478e-02  1.1161555e-02\n",
            "   -3.6181927e-02  4.2236995e-02]\n",
            "  [-1.2169611e-02 -3.2943390e-02 -2.3275947e-02  1.0270953e-02\n",
            "   -2.7363062e-02 -2.8315032e-02  1.5833106e-02  2.0464659e-03\n",
            "   -3.4729086e-02  2.0253424e-02]\n",
            "  [-1.3693117e-02  3.4608271e-02  4.6674732e-02  3.9484892e-02\n",
            "    1.2077391e-05  2.7506087e-02 -4.0843308e-02 -2.2407210e-02\n",
            "   -3.4054626e-02 -2.5977029e-02]\n",
            "  [ 2.5185123e-03  3.2774281e-02  4.9799684e-02  4.9093854e-02\n",
            "   -3.5421669e-02  1.2114607e-02 -1.3848700e-02 -1.9787991e-02\n",
            "   -5.8520436e-03  3.2433782e-02]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwkhaR4dPS64",
        "outputId": "e629aab1-870a-4116-f4cb-99e9d7c6aeae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,    0,    0,    0, 8341, 9030, 6657, 7169], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.predict(embedded_docs)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEV_YK_0PTv6",
        "outputId": "1487092a-b148-4ee7-8627-7058ce1a18f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "[[-0.0221261   0.04590095  0.0117821   0.0123082  -0.00669001  0.03239172\n",
            "  -0.03227305 -0.016047   -0.00566549  0.02157482]\n",
            " [-0.0221261   0.04590095  0.0117821   0.0123082  -0.00669001  0.03239172\n",
            "  -0.03227305 -0.016047   -0.00566549  0.02157482]\n",
            " [-0.0221261   0.04590095  0.0117821   0.0123082  -0.00669001  0.03239172\n",
            "  -0.03227305 -0.016047   -0.00566549  0.02157482]\n",
            " [-0.0221261   0.04590095  0.0117821   0.0123082  -0.00669001  0.03239172\n",
            "  -0.03227305 -0.016047   -0.00566549  0.02157482]\n",
            " [ 0.01943633  0.043055    0.00371826 -0.01466006  0.0415856   0.02257473\n",
            "   0.0442146   0.0372561  -0.04177881  0.03253928]\n",
            " [ 0.047121   -0.03629162  0.00603875  0.03375593 -0.03179248  0.04071374\n",
            "   0.00441973 -0.01862949  0.03250908  0.00180762]\n",
            " [ 0.01062462 -0.04493208 -0.02328709 -0.03729993  0.02037457  0.02284164\n",
            "   0.04222066  0.03588844  0.04060311 -0.02541548]\n",
            " [ 0.04528215 -0.01976923 -0.01649962  0.03962335 -0.01314211 -0.01247604\n",
            "  -0.01811049 -0.04277443 -0.03726593 -0.03041228]]\n"
          ]
        }
      ]
    }
  ]
}